{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimSwap_images.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "xTo1C8aPUoOh",
        "rcoRX8OnVImS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_gtFoV8BuRx"
      },
      "source": [
        "# SimSwap\n",
        "Reference: https://github.com/neuralchen/SimSwap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4fUfjQYUlYz"
      },
      "source": [
        "## Prepare code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA_4CeWZCHLP"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/woctezuma/SimSwap.git\n",
        "%cd /content/SimSwap/\n",
        "!git checkout upgrade-insightface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5K4au_UCkKn"
      },
      "source": [
        "!pip install insightface==0.7.3 onnxruntime moviepy > /dev/null\n",
        "!pip install googledrivedownloader > /dev/null\n",
        "!pip install imageio==2.34.0 > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTo1C8aPUoOh"
      },
      "source": [
        "## Prepare models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQGu8shAWOPE"
      },
      "source": [
        "%cd /content/SimSwap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLti1J0pEFjJ"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader\n",
        "\n",
        "!wget -P ./arcface_model https://github.com/woctezuma/SimSwap-colab/releases/download/1.0/arcface_checkpoint.tar\n",
        "!wget https://github.com/neuralchen/SimSwap/releases/download/1.0/checkpoints.zip\n",
        "!wget -P ./parsing_model/checkpoint https://github.com/neuralchen/SimSwap/releases/download/1.0/79999_iter.pth\n",
        "!wget https://github.com/neuralchen/SimSwap/releases/download/512_beta/512.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iwPorp0Mnvw"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        " https://sh23tw.dm.files.1drv.com/y4mmGiIkNVigkSwOKDcV3nwMJulRGhbtHdkheehR5TArc52UjudUYNXAEvKCii2O5LAmzGCGK6IfleocxuDeoKxDZkNzDRSt4ZUlEt8GlSOpCXAFEkBwaZimtWGDRbpIGpb_pz9Nq5jATBQpezBS6G_UtspWTkgrXHHxhviV2nWy8APPx134zOZrUIbkSF6xnsqzs3uZ_SEX_m9Rey0ykpx9w \\\n",
        " -O antelope.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKAcJ4ngVEUV"
      },
      "source": [
        "!unzip ./checkpoints.zip  -d ./checkpoints\n",
        "\n",
        "!unzip 512.zip -d ./checkpoints\n",
        "\n",
        "!unzip antelope.zip -d ./insightface_func/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZD8GPHsU4lN"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3YlcuLdVyZg"
      },
      "source": [
        "### Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8SAi2SYSFmN"
      },
      "source": [
        "%cd /content\n",
        "\n",
        "!wget https://i.imgur.com/QYJOzy7.jpeg -O cpc_ackboo.jpg\n",
        "!wget https://i.imgur.com/l5MGOws.jpeg -O starwars_meme.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_fname = '/content/person5.JPG'\n",
        "output_fname = '/content/image3.jpg'"
      ],
      "metadata": {
        "id": "xwzIouURmoFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-y81Z5dV3pR"
      },
      "source": [
        "# input_fname = '/content/cpc_ackboo.jpg'\n",
        "# output_fname = '/content/starwars_meme.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcoRX8OnVImS"
      },
      "source": [
        "### Convert to JPG\n",
        "\n",
        "Images should not be too large, hence the (arbitrary) limitation of 1024 length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLf-l1tjTY0W"
      },
      "source": [
        "def get_new_size(img_size,\n",
        "                 max_allowed_length = 1024):\n",
        "\n",
        "  if any(max_allowed_length < sz for sz in img_size):\n",
        "    long_length = max(img_size)\n",
        "    ratio = max_allowed_length / long_length\n",
        "  else:\n",
        "    ratio = 1.0\n",
        "\n",
        "  new_img_size = [\n",
        "                  int(ratio*sz)\n",
        "                  for sz in img_size\n",
        "                  ]\n",
        "\n",
        "  return tuple(new_img_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEOmN-DdPB5g"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "allow_resize = False\n",
        "\n",
        "for fname in [input_fname, output_fname]:\n",
        "  jpg_fname = fname.replace('.png', '.jpg')\n",
        "\n",
        "  try:\n",
        "    img = Image.open(fname)\n",
        "  except FileNotFoundError:\n",
        "    continue\n",
        "\n",
        "  new_size = get_new_size(img.size, max_allowed_length = 1024)\n",
        "  if allow_resize:\n",
        "    print(f'Resizing from {img.size} to {new_size}')\n",
        "    img.resize(new_size)\n",
        "\n",
        "  print(f'Saving to {jpg_fname}')\n",
        "  img.convert('RGB').save(jpg_fname)\n",
        "\n",
        "jpg_input = input_fname.replace('.png', '.jpg')\n",
        "jpg_output = output_fname.replace('.png', '.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch opencv-python"
      ],
      "metadata": {
        "id": "JylRrgDbHlKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZBoCtU9U8B3"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ_ZBtDQVLco"
      },
      "source": [
        "### Single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zywghoUNOAmS"
      },
      "source": [
        "%cd /content/SimSwap\n",
        "%mkdir -p /content/output/single/\n",
        "\n",
        "!python test_wholeimage_swapsingle.py \\\n",
        " --no_simswaplogo \\\n",
        " --use_mask \\\n",
        " --crop_size 512 \\\n",
        " --isTrain false  --name people \\\n",
        " --Arc_path arcface_model/arcface_checkpoint.tar \\\n",
        " --pic_a_path {jpg_input} \\\n",
        " --pic_b_path {jpg_output} \\\n",
        " --output_path /content/output/single/ > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install insightface opencv-python matplotlib"
      ],
      "metadata": {
        "id": "2QXsi8K2Ui0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def face_landmark(image_path):\n",
        "    # YOLOv8\n",
        "    model = YOLO('yolov8n.pt')\n",
        "\n",
        "    # ID of person in YOLO8\n",
        "    person_class_id = [key for key, value in model.names.items() if value == 'person'][0]\n",
        "\n",
        "    # mediapipe for face landmarks\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5)\n",
        "\n",
        "    # FACE DETECTION BY yolov8\n",
        "    image = cv2.imread(image_path)\n",
        "    results = model(image)\n",
        "\n",
        "    # boundingbox + landmark\n",
        "    for result in results[0].boxes:\n",
        "        if result.cls == person_class_id:  #just person\n",
        "            x1, y1, x2, y2 = map(int, result.xyxy[0])\n",
        "\n",
        "            # face cropping\n",
        "            face_roi = image[y1:y2, x1:x2]\n",
        "\n",
        "            # convert img to RGB for mediapipe\n",
        "            rgb_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # face landmark\n",
        "            results_face_mesh = face_mesh.process(rgb_face)\n",
        "\n",
        "            # 5 landmark of face\n",
        "            left_eye_points = [133, 33]     # گوشه‌های چشم چپ\n",
        "            right_eye_points = [362, 263]   # گوشه‌های چشم راست\n",
        "            nose_tip = 1                    # نوک بینی\n",
        "            left_mouth_corner = 61          # گوشه چپ لب\n",
        "            right_mouth_corner = 291        # گوشه راست لب\n",
        "\n",
        "            if results_face_mesh.multi_face_landmarks:\n",
        "                for face_landmarks in results_face_mesh.multi_face_landmarks:\n",
        "                    # center of face\n",
        "                    center_x = (x1 + x2) // 2\n",
        "                    center_y = (y1 + y2) // 2\n",
        "\n",
        "                    # center of left eye\n",
        "                    left_eye_center = [\n",
        "                        sum(face_landmarks.landmark[idx].x for idx in left_eye_points) / 2,\n",
        "                        sum(face_landmarks.landmark[idx].y for idx in left_eye_points) / 2\n",
        "                    ]\n",
        "                    left_eye_x = int(left_eye_center[0] * face_roi.shape[1]) + x1\n",
        "                    left_eye_y = int(left_eye_center[1] * face_roi.shape[0]) + y1\n",
        "                    cv2.circle(image, (left_eye_x, left_eye_y), 3, (0, 255, 0), -1)\n",
        "\n",
        "                    # center of right eye\n",
        "                    right_eye_center = [\n",
        "                        sum(face_landmarks.landmark[idx].x for idx in right_eye_points) / 2,\n",
        "                        sum(face_landmarks.landmark[idx].y for idx in right_eye_points) / 2\n",
        "                    ]\n",
        "                    right_eye_x = int(right_eye_center[0] * face_roi.shape[1]) + x1\n",
        "                    right_eye_y = int(right_eye_center[1] * face_roi.shape[0]) + y1\n",
        "                    cv2.circle(image, (right_eye_x, right_eye_y), 3, (0, 255, 0), -1)\n",
        "\n",
        "                    # nose\n",
        "                    nose = face_landmarks.landmark[nose_tip]\n",
        "                    nose_x = int(nose.x * face_roi.shape[1]) + x1\n",
        "                    nose_y = int(nose.y * face_roi.shape[0]) + y1\n",
        "                    cv2.circle(image, (nose_x, nose_y), 3, (0, 255, 0), -1)\n",
        "\n",
        "                    # left mouth\n",
        "                    left_mouth = face_landmarks.landmark[left_mouth_corner]\n",
        "                    left_mouth_x = int(left_mouth.x * face_roi.shape[1]) + x1\n",
        "                    left_mouth_y = int(left_mouth.y * face_roi.shape[0]) + y1\n",
        "                    cv2.circle(image, (left_mouth_x, left_mouth_y), 3, (0, 255, 0), -1)\n",
        "\n",
        "                    # right mouth\n",
        "                    right_mouth = face_landmarks.landmark[right_mouth_corner]\n",
        "                    right_mouth_x = int(right_mouth.x * face_roi.shape[1]) + x1\n",
        "                    right_mouth_y = int(right_mouth.y * face_roi.shape[0]) + y1\n",
        "                    cv2.circle(image, (right_mouth_x, right_mouth_y), 3, (0, 255, 0), -1)\n",
        "\n",
        "    results = [left_eye_x, left_eye_y, right_eye_x, right_eye_y, nose_x, nose_y, left_mouth_x, left_mouth_y, right_mouth_x, right_mouth_y]\n",
        "    # plot image by landmarks\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return(results)\n",
        "\n",
        "\n",
        "image_path = '/content/image1.png'\n",
        "landmarks = face_landmark(image_path)"
      ],
      "metadata": {
        "id": "7scyeUKsUurq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt2GdF2J0fRx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !git clone https://github.com/hpc203/yolov8-face-landmarks-opencv-dnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/yolov8-face-landmarks-opencv-dnn/main.py --imgpath '/content/image2.png' --modelpath '/content/drive/MyDrive/yolov8-face-landmarks-opencv-dnn/weights/yolov8n-face.onnx'"
      ],
      "metadata": {
        "id": "dM9N7rsMqAzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "class YOLOv8_face:\n",
        "    def __init__(self, path, conf_thres=0.2, iou_thres=0.5):\n",
        "        self.conf_threshold = conf_thres\n",
        "        self.iou_threshold = iou_thres\n",
        "        self.class_names = ['face']\n",
        "        self.num_classes = len(self.class_names)\n",
        "        # Initialize model\n",
        "        self.net = cv2.dnn.readNet(path)\n",
        "        self.input_height = 640\n",
        "        self.input_width = 640\n",
        "        self.reg_max = 16\n",
        "\n",
        "        self.project = np.arange(self.reg_max)\n",
        "        self.strides = (8, 16, 32)\n",
        "        self.feats_hw = [(math.ceil(self.input_height / self.strides[i]), math.ceil(self.input_width / self.strides[i])) for i in range(len(self.strides))]\n",
        "        self.anchors = self.make_anchors(self.feats_hw)\n",
        "\n",
        "    def make_anchors(self, feats_hw, grid_cell_offset=0.5):\n",
        "        \"\"\"Generate anchors from features.\"\"\"\n",
        "        anchor_points = {}\n",
        "        for i, stride in enumerate(self.strides):\n",
        "            h, w = feats_hw[i]\n",
        "            x = np.arange(0, w) + grid_cell_offset  # shift x\n",
        "            y = np.arange(0, h) + grid_cell_offset  # shift y\n",
        "            sx, sy = np.meshgrid(x, y)\n",
        "            anchor_points[stride] = np.stack((sx, sy), axis=-1).reshape(-1, 2)\n",
        "        return anchor_points\n",
        "\n",
        "    def softmax(self, x, axis=1):\n",
        "        x_exp = np.exp(x)\n",
        "        x_sum = np.sum(x_exp, axis=axis, keepdims=True)\n",
        "        s = x_exp / x_sum\n",
        "        return s\n",
        "\n",
        "    def resize_image(self, srcimg, keep_ratio=True):\n",
        "        top, left, newh, neww = 0, 0, self.input_width, self.input_height\n",
        "        if keep_ratio and srcimg.shape[0] != srcimg.shape[1]:\n",
        "            hw_scale = srcimg.shape[0] / srcimg.shape[1]\n",
        "            if hw_scale > 1:\n",
        "                newh, neww = self.input_height, int(self.input_width / hw_scale)\n",
        "                img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n",
        "                left = int((self.input_width - neww) * 0.5)\n",
        "                img = cv2.copyMakeBorder(img, 0, 0, left, self.input_width - neww - left, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "            else:\n",
        "                newh, neww = int(self.input_height * hw_scale), self.input_width\n",
        "                img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n",
        "                top = int((self.input_height - newh) * 0.5)\n",
        "                img = cv2.copyMakeBorder(img, top, self.input_height - newh - top, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "        else:\n",
        "            img = cv2.resize(srcimg, (self.input_width, self.input_height), interpolation=cv2.INTER_AREA)\n",
        "        return img, newh, neww, top, left\n",
        "\n",
        "    def detect(self, srcimg):\n",
        "        input_img, newh, neww, padh, padw = self.resize_image(cv2.cvtColor(srcimg, cv2.COLOR_BGR2RGB))\n",
        "        scale_h, scale_w = srcimg.shape[0] / newh, srcimg.shape[1] / neww\n",
        "        input_img = input_img.astype(np.float32) / 255.0\n",
        "\n",
        "        blob = cv2.dnn.blobFromImage(input_img)\n",
        "        self.net.setInput(blob)\n",
        "        outputs = self.net.forward(self.net.getUnconnectedOutLayersNames())\n",
        "\n",
        "        det_bboxes, det_conf, det_classid, landmarks = self.post_process(outputs, scale_h, scale_w, padh, padw)\n",
        "        return det_bboxes, det_conf, det_classid, landmarks\n",
        "\n",
        "    def post_process(self, preds, scale_h, scale_w, padh, padw):\n",
        "        bboxes, scores, landmarks = [], [], []\n",
        "        for i, pred in enumerate(preds):\n",
        "            stride = int(self.input_height / pred.shape[2])\n",
        "            pred = pred.transpose((0, 2, 3, 1))\n",
        "\n",
        "            box = pred[..., :self.reg_max * 4]\n",
        "            cls = 1 / (1 + np.exp(-pred[..., self.reg_max * 4:-15])).reshape((-1, 1))\n",
        "            kpts = pred[..., -15:].reshape((-1, 15))\n",
        "\n",
        "            tmp = box.reshape(-1, 4, self.reg_max)\n",
        "            bbox_pred = self.softmax(tmp, axis=-1)\n",
        "            bbox_pred = np.dot(bbox_pred, self.project).reshape((-1, 4))\n",
        "\n",
        "            bbox = self.distance2bbox(self.anchors[stride], bbox_pred, max_shape=(self.input_height, self.input_width)) * stride\n",
        "            kpts[:, 0::3] = (kpts[:, 0::3] * 2.0 + (self.anchors[stride][:, 0].reshape((-1, 1)) - 0.5)) * stride\n",
        "            kpts[:, 1::3] = (kpts[:, 1::3] * 2.0 + (self.anchors[stride][:, 1].reshape((-1, 1)) - 0.5)) * stride\n",
        "            kpts[:, 2::3] = 1 / (1 + np.exp(-kpts[:, 2::3]))\n",
        "\n",
        "            bbox -= np.array([[padw, padh, padw, padh]])\n",
        "            bbox *= np.array([[scale_w, scale_h, scale_w, scale_h]])\n",
        "            kpts -= np.tile(np.array([padw, padh, 0]), 5).reshape((1, 15))\n",
        "            kpts *= np.tile(np.array([scale_w, scale_h, 1]), 5).reshape((1, 15))\n",
        "\n",
        "            bboxes.append(bbox)\n",
        "            scores.append(cls)\n",
        "            landmarks.append(kpts)\n",
        "\n",
        "        bboxes = np.concatenate(bboxes, axis=0)\n",
        "        scores = np.concatenate(scores, axis=0)\n",
        "        landmarks = np.concatenate(landmarks, axis=0)\n",
        "\n",
        "        bboxes_wh = bboxes.copy()\n",
        "        bboxes_wh[:, 2:4] = bboxes[:, 2:4] - bboxes[:, 0:2]  # xywh\n",
        "        classIds = np.argmax(scores, axis=1)\n",
        "        confidences = np.max(scores, axis=1)\n",
        "\n",
        "        mask = confidences > self.conf_threshold\n",
        "        bboxes_wh = bboxes_wh[mask]\n",
        "        confidences = confidences[mask]\n",
        "        classIds = classIds[mask]\n",
        "        landmarks = landmarks[mask]\n",
        "\n",
        "        indices = cv2.dnn.NMSBoxes(bboxes_wh.tolist(), confidences.tolist(), self.conf_threshold, self.iou_threshold).flatten()\n",
        "        if len(indices) > 0:\n",
        "            mlvl_bboxes = bboxes_wh[indices]\n",
        "            confidences = confidences[indices]\n",
        "            classIds = classIds[indices]\n",
        "            landmarks = landmarks[indices]\n",
        "            return mlvl_bboxes, confidences, classIds, landmarks\n",
        "        else:\n",
        "            print('nothing detect')\n",
        "            return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    def distance2bbox(self, points, distance, max_shape=None):\n",
        "        x1 = points[:, 0] - distance[:, 0]\n",
        "        y1 = points[:, 1] - distance[:, 1]\n",
        "        x2 = points[:, 0] + distance[:, 2]\n",
        "        y2 = points[:, 1] + distance[:, 3]\n",
        "        if max_shape is not None:\n",
        "            x1 = np.clip(x1, 0, max_shape[1])\n",
        "            y1 = np.clip(y1, 0, max_shape[0])\n",
        "            x2 = np.clip(x2, 0, max_shape[1])\n",
        "            y2 = np.clip(y2, 0, max_shape[0])\n",
        "        return np.stack([x1, y1, x2, y2], axis=-1)\n",
        "\n",
        "    def draw_detections(self, image, boxes, scores, kpts):\n",
        "        for box, score, kp in zip(boxes, scores, kpts):\n",
        "            x, y, w, h = box.astype(int)\n",
        "            # Draw rectangle\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), thickness=3)\n",
        "            cv2.putText(image, \"face:\" + str(round(score, 2)), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), thickness=2)\n",
        "            for i in range(5):\n",
        "                cv2.circle(image, (int(kp[i * 3]), int(kp[i * 3 + 1])), 4, (0, 255, 0), thickness=-1)\n",
        "        return image\n",
        "\n",
        "\n",
        "def run_face_detection(img_path, model_path):\n",
        "    yolo_face_detector = YOLOv8_face(model_path)\n",
        "    image = cv2.imread(img_path)\n",
        "    boxes, scores, classIds, landmarks = yolo_face_detector.detect(image)\n",
        "    output_image = yolo_face_detector.draw_detections(image.copy(), boxes, scores, landmarks)\n",
        "    cv2_imshow(output_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "img_path = '/content/image2.png'\n",
        "model_path = '/content/drive/MyDrive/yolov8-face-landmarks-opencv-dnn/weights/yolov8n-face.onnx'\n",
        "run_face_detection(img_path, model_path)"
      ],
      "metadata": {
        "id": "t0uxoU8ZyjwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MxvjygsVNb4"
      },
      "source": [
        "### Multi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PudoS9JBRsOs"
      },
      "source": [
        "%cd /content/SimSwap\n",
        "%mkdir -p /content/output/multi/\n",
        "\n",
        "!python test_wholeimage_swapmulti.py \\\n",
        " --no_simswaplogo \\\n",
        " --use_mask \\\n",
        " --crop_size 512 \\\n",
        " --isTrain false  --name people \\\n",
        " --Arc_path arcface_model/arcface_checkpoint.tar \\\n",
        " --pic_a_path {jpg_input} \\\n",
        " --pic_b_path {jpg_output} \\\n",
        " --output_path /content/output/multi/ > /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}